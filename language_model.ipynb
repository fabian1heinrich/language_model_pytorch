{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# train models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## wikitext2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "from model import TransformerModel, train_model, evaluate_model, save_model\n",
    "from data_provider import LMData\n",
    "\n",
    "vocab_iter = WikiText2(split='train')\n",
    "train_iter, eval_iter, test_iter = WikiText2()\n",
    "train_data = LMData(vocab_iter, train_iter, 20, 35)\n",
    "test_data = LMData(vocab_iter, test_iter, 20, 35)\n",
    "\n",
    "n_token = train_data.n_token  # size of vocabulary\n",
    "model = TransformerModel(n_token)\n",
    "\n",
    "#train_model(model, train_data, 10)\n",
    "#save_model(model)\n",
    "\n",
    "test_loss = evaluate_model(model, test_data)\n",
    "print('perplexity on test data: {:.2f}'.format(math.exp(test_loss)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## wikitext103"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\n",
    "\n",
    "from torchtext.datasets import WikiText103\n",
    "\n",
    "from model import TransformerModel, train_model, evaluate_model, save_model\n",
    "from data_provider import LMData\n",
    "\n",
    "vocab_iter = WikiText103(split='train')\n",
    "train_iter, eval_iter, test_iter = WikiText103()\n",
    "train_data = LMData(vocab_iter, train_iter, 20, 35)\n",
    "test_data = LMData(vocab_iter, test_iter, 20, 35)\n",
    "\n",
    "n_token = train_data.n_token  # size of vocabulary\n",
    "model = TransformerModel(n_token)\n",
    "\n",
    "#train_model(model, train_data, 10)\n",
    "#save_model(model)\n",
    "\n",
    "test_loss = evaluate_model(model, test_data)\n",
    "print('perplexity on test data: {:.2f}'.format(math.exp(test_loss)))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2516022/872844319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvocab_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikiText103\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikiText103\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/language_model_pytorch/data_provider/lm_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_iter, data_iter, batch_size, seq_len, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/language_model_pytorch/data_provider/_create_vocab.py\u001b[0m in \u001b[0;36mcreate_vocab\u001b[0;34m(raw_text_iter, tokenizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     vocab = build_vocab_from_iterator(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/language_model_pytorch/.venv/lib/python3.8/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/language_model_pytorch/.venv/lib/python3.8/site-packages/torchtext/data/datasets_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_pos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_lines\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## librispeech"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from model import TransformerModel, save_model, train_model, evaluate_model\n",
    "from data_provider import LibriSpeechRawIter, LMData\n",
    "\n",
    "train_iter = LibriSpeechRawIter('train')\n",
    "train_data = LMData(train_iter, 20, 35)\n",
    "test_iter = LibriSpeechRawIter('test')\n",
    "test_data = LMData(test_iter, 20, 35)\n",
    "n_token = train_data.n_token\n",
    "\n",
    "model = TransformerModel(n_token)\n",
    "train_model(model, train_data, 10)\n",
    "save_model(model)\n",
    "test_loss = evaluate_model(model, test_data)\n",
    "\n",
    "print('perplexity on test dataset: {:.2f}'.format(math.exp(test_loss)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## airbus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load pretrained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "from torchtext.datasets import WikiText2, WikiText103, PennTreebank\n",
    "\n",
    "\n",
    "from model import TransformerModel\n",
    "from data_provider import LMData, LibriSpeechRawIter\n",
    "\n",
    "vocab_iter = WikiText103(split='train')\n",
    "train_iter, eval_iter, test_iter = WikiText103()\n",
    "test_data = LMData(vocab_iter, test_iter, 20, 35)\n",
    "n_token = test_data.n_token\n",
    "\n",
    "path = 'saved_models/20210917_102159_IXOYXWR' # wiki103\n",
    "model_dict = torch.load(path)\n",
    "model = TransformerModel(n_token=n_token)\n",
    "model.load_state_dict(model_dict['model_state_dict'])\n",
    "model.eval();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "from data_provider import str2seq, seq2str\n",
    "from model import inference\n",
    "\n",
    "input_str = 'i want to get the song'\n",
    "input = str2seq(input_str, test_data.vocab, test_data.tokenizer)\n",
    "input_length = 6\n",
    "\n",
    "output = inference(model, input[:input_length])\n",
    "prediction = torch.argmax(output, dim=1)\n",
    "prediction_str = seq2str(prediction.unsqueeze_(dim=1)[-1], test_data.vocab)\n",
    "\n",
    "print(input_str)\n",
    "print(seq2str(input[:input_length], test_data.vocab))\n",
    "print(prediction_str)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "import torch\n",
    "import jiwer\n",
    "\n",
    "from model import inference\n",
    "from data_provider import seq2str\n",
    "\n",
    "model.eval()\n",
    "wer = 0\n",
    "for n_batches, batch in enumerate(test_data):\n",
    "    #print(seq2str(inputs[:, 0], test_data.vocab))\n",
    "    #print(seq2str(targets[::test_data.batch_size], test_data.vocab))\n",
    "    inputs, targets, index = batch\n",
    "    output_prob = inference(model, inputs)\n",
    "    output_max = torch.argmax(output_prob, dim=1)\n",
    "    output_max = output_max.reshape(35, 20)\n",
    "    targets = targets.reshape(35, 20)\n",
    "\n",
    "    for n_samples, sample in enumerate(zip(output_max.t(), targets.t())):\n",
    "        prediction, target = sample\n",
    "        prediction_str = seq2str(prediction, test_data.vocab)\n",
    "        target_str = seq2str(target, test_data.vocab)\n",
    "        wer += jiwer.wer(target_str, prediction_str)\n",
    "\n",
    "print(wer/(n_batches * n_samples))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8398100514444204\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "56aff06483e8080fc599db45bad96297ade79cfb8e88877c9ba072ec0be460cd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}